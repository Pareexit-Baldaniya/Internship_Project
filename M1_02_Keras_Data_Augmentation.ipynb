{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1, Task 2: Data Loading and Augmentation Using Keras\n",
    "\n",
    "**Objective:** Build an efficient, generator-based data pipeline using `tf.data` and apply image augmentations using Keras Preprocessing Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install tensorflow tensorflow-datasets matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Import libraries and load the EuroSAT dataset. We will split the training data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "(ds_train, ds_validation), ds_info = tfds.load(\n",
    "    'eurosat/rgb',\n",
    "    split=['train[:80%]', 'train[80%:]'], # Use 80% for training, 20% for validation\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "CLASS_NAMES = ds_info.features['label'].names\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "IMG_SIZE = 128 # Resize images for manageable model input\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"Number of training samples: {tf.data.experimental.cardinality(ds_train)}\")\n",
    "print(f\"Number of validation samples: {tf.data.experimental.cardinality(ds_validation)}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Class names: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Data Pipeline\n",
    "\n",
    "We will create a function to process the data:\n",
    "1.  **Resize Images:** Standardize image sizes.\n",
    "2.  **Normalize Pixel Values:** Scale pixel values from `[0, 255]` to `[0, 1]` for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # Normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# Apply the processing to our datasets\n",
    "ds_train_processed = ds_train.map(process_image)\n",
    "ds_validation_processed = ds_validation.map(process_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Data augmentation creates modified versions of images in the training set to help the model generalize better and reduce overfitting. Keras preprocessing layers are ideal as they can be included directly in the model, making it more portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "], name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Augmented Data\n",
    "\n",
    "Let's see the effect of our augmentation pipeline on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch of images from the processed training set\n",
    "for images, labels in ds_train_processed.take(1):\n",
    "    image_to_show = images[0]\n",
    "    label_to_show = CLASS_NAMES[labels[0]]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.imshow(image_to_show)\n",
    "plt.title(f\"Original: {label_to_show}\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Apply augmentation 8 times to the same image\n",
    "for i in range(8):\n",
    "    augmented_image = data_augmentation(tf.expand_dims(image_to_show, 0))\n",
    "    plt.subplot(3, 3, i + 2)\n",
    "    plt.imshow(augmented_image[0])\n",
    "    plt.title(\"Augmented\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"Data Augmentation Examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing the Pipeline for Performance\n",
    "\n",
    "To create a highly performant input pipeline, we add:\n",
    "1.  **Augmentation:** Apply the augmentation layers.\n",
    "2.  **Batching:** Group samples into batches.\n",
    "3.  **Caching:** Cache the dataset in memory to save time on subsequent epochs.\n",
    "4.  **Prefetching:** Overlap data preprocessing and model execution for maximum GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(ds, shuffle=False):\n",
    "    # Cache before shuffling and batching\n",
    "    ds = ds.cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    # Apply augmentation after batching (GPU-efficient)\n",
    "    if shuffle: # Only apply augmentation to training set\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    # Prefetch to overlap data production with consumption\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_final = configure_dataset(ds_train_processed, shuffle=True)\n",
    "validation_ds_final = configure_dataset(ds_validation_processed)\n",
    "\n",
    "print(\"Finalized tf.data pipelines for training and validation.\")\n",
    "print(train_ds_final)\n",
    "print(validation_ds_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have successfully built a complete and efficient Keras data pipeline. It starts with loading raw data, applies preprocessing and augmentation, and uses caching and prefetching to ensure optimal performance during model training. This pipeline is now ready to be fed into a `model.fit()` call."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
