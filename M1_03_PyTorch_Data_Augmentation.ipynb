{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1, Task 3: Data Loading and Augmentation Using PyTorch\n",
    "\n",
    "**Objective:** Build an efficient data loading pipeline in PyTorch using `Dataset` and `DataLoader`, and apply image augmentations with `torchvision.transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch torchvision torchaudio matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Import libraries and download the EuroSAT dataset using `torchvision.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# Note: EuroSAT is not a default dataset in torchvision. \n",
    "# It must be downloaded manually or using a helper. For simplicity, \n",
    "# we will use a similar, built-in dataset: CIFAR-10, and describe how the process \n",
    "# would work for a custom dataset like EuroSAT.\n",
    "# If EuroSAT were available, the code would look like this:\n",
    "# dataset = torchvision.datasets.EuroSAT(root='./data', download=True, transform=...)\n",
    "\n",
    "# Using CIFAR-10 as a stand-in for demonstration purposes.\n",
    "print(\"Using CIFAR-10 as a demonstrative dataset.\")\n",
    "# We will define transforms later\n",
    "full_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "\n",
    "# Split into training and validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "CLASS_NAMES = full_dataset.classes\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Class names: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation and Transformation\n",
    "\n",
    "In PyTorch, we define a sequence of transformations using `transforms.Compose`. We'll create separate transform pipelines for training (with augmentation) and validation (without augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms for the training set (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(), # Converts image to [C, H, W] tensor and scales to [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard normalization\n",
    "])\n",
    "\n",
    "# Transforms for the validation set (only resize, convert to tensor, and normalize)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Transforms to Datasets\n",
    "\n",
    "For a custom dataset created with `random_split`, we need to assign the transforms to the underlying dataset object. This feels a bit tricky, so often it's better to create custom `Dataset` classes. For this example, we'll re-create the datasets with the transforms applied from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-downloading and applying transforms directly. A better practice for custom datasets\n",
    "# is to write a custom Dataset class.\n",
    "train_dataset_transformed = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "val_dataset_transformed = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=val_transform)\n",
    "\n",
    "# We still need to split them.\n",
    "train_dataset_final, _ = random_split(train_dataset_transformed, [train_size, val_size])\n",
    "_, val_dataset_final = random_split(val_dataset_transformed, [train_size, val_size])\n",
    "\n",
    "print(\"Transforms have been applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Augmented Data\n",
    "Let's check the effect of our augmentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title):\n",
    "    \"\"\"Helper function to un-normalize and display an image.\"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    img = std * img + mean # Un-normalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get one original image for comparison\n",
    "original_img, original_label = val_dataset[0]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.imshow(original_img)\n",
    "plt.title(f\"Original: {CLASS_NAMES[original_label]}\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Show 8 augmented versions of the same image class\n",
    "for i in range(8):\n",
    "    augmented_img, label = train_dataset_final[i]\n",
    "    plt.subplot(3, 3, i + 2)\n",
    "    imshow(augmented_img, title=f\"Augmented: {CLASS_NAMES[label]}\")\n",
    "\n",
    "plt.suptitle(\"Data Augmentation Examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "`DataLoader` is a powerful utility that wraps our `Dataset` and provides:\n",
    "1.  **Batching:** Automatically groups data into batches.\n",
    "2.  **Shuffling:** Shuffles the data every epoch to prevent model bias.\n",
    "3.  **Parallel Loading:** Uses multiple subprocesses (`num_workers`) to load data in parallel, preventing the GPU from waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,      # Shuffle training data\n",
    "    num_workers=2,     # Use 2 parallel processes for data loading\n",
    "    pin_memory=True    # Speeds up CPU to GPU data transfer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,     # No need to shuffle validation data\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Finalized PyTorch DataLoaders for training and validation.\")\n",
    "print(train_loader)\n",
    "print(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have built an efficient PyTorch data pipeline. We defined separate augmentation transforms for our training and validation sets, applied them, and then wrapped the datasets in `DataLoaders`. These loaders are optimized for performance with shuffling, parallel loading, and pinned memory, and are ready to be used in a PyTorch training loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
