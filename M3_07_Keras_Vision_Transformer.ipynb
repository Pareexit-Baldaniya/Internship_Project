{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3, Task 7: Vision Transformers in Keras\n",
    "\n",
    "**Objective:** Implement, train, and evaluate a Vision Transformer (ViT) model for image classification using TensorFlow and Keras, and compare its performance to the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install tensorflow tensorflow-datasets matplotlib scikit-learn tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Vision Transformers (ViT)\n",
    "\n",
    "The Vision Transformer (ViT) is a model that applies the Transformer architecture, originally successful in Natural Language Processing (NLP), to computer vision tasks. It works as follows:\n",
    "\n",
    "1.  **Image Patching:** The input image is split into a sequence of fixed-size, non-overlapping patches.\n",
    "2.  **Linear Projection:** Each patch is flattened and linearly projected into an embedding vector.\n",
    "3.  **Positional Embeddings:** Position embeddings are added to the patch embeddings to retain spatial information.\n",
    "4.  **Transformer Encoder:** The resulting sequence of vectors is fed into a standard Transformer Encoder, which uses self-attention mechanisms to learn relationships between patches.\n",
    "5.  **Classification Head:** The output corresponding to a special `[CLS]` token is passed through a classifier (MLP head) to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Data Loading and Preprocessing (Reused from M2_04) ---\n",
    "(ds_train, ds_validation), ds_info = tfds.load(\n",
    "    'eurosat/rgb', split=['train[:80%]', 'train[80%:]'],\n",
    "    shuffle_files=True, as_supervised=True, with_info=True)\n",
    "\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "IMG_SIZE = 72  # ViT models often work better with smaller image sizes like 72, 224, etc.\n",
    "BATCH_SIZE = 128 # ViTs are memory intensive, might need a smaller batch size\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def process_image(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "data_augmentation = keras.Sequential([...]) # Same as before, can be defined if needed\n",
    "\n",
    "def configure_dataset(ds, shuffle=False):\n",
    "    ds = ds.map(process_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = configure_dataset(ds_train, shuffle=True)\n",
    "validation_ds = configure_dataset(ds_validation)\n",
    "print(\"Data pipelines are ready for ViT.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ViT Model\n",
    "We will implement a simplified ViT model from scratch using Keras layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "PATCH_SIZE = 6\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_UNITS = [PROJECTION_DIM * 2, PROJECTION_DIM]\n",
    "TRANSFORMER_LAYERS = 4\n",
    "MLP_HEAD_UNITS = [2048, 1024]\n",
    "\n",
    "# --- Keras Layers Implementation ---\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    # Create patches.\n",
    "    patches = Patches(PATCH_SIZE)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(NUM_PATCHES, PROJECTION_DIM)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(NUM_CLASSES, activation=\"softmax\")(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "vit_model = create_vit_classifier()\n",
    "vit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ViT Model\n",
    "ViT models typically require longer training times and more data to converge compared to CNNs. We'll use the AdamW optimizer, which often works well for Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20 # ViT may need more epochs\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "vit_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = vit_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Let's evaluate our trained ViT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('ViT Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('ViT Loss')\n",
    "plt.show()\n",
    "\n",
    "# Detailed Classification Report\n",
    "y_true = np.concatenate([y for x, y in validation_ds], axis=0)\n",
    "y_pred_probs = vit_model.predict(validation_ds)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "class_names = ds_info.features['label'].names\n",
    "\n",
    "print(\"\\nViT Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: ViT vs. CNN\n",
    "\n",
    "- **Performance:** On smaller datasets like EuroSAT, a well-tuned CNN (from M2_04) might outperform a ViT trained from scratch. ViTs are data-hungry and truly shine when pre-trained on massive datasets (like ImageNet-21k) and then fine-tuned.\n",
    "- **Architecture:** ViTs lack the built-in inductive biases of CNNs (like translation equivariance and locality), which makes them more flexible but harder to train on smaller datasets.\n",
    "- **Complexity:** Implementing a ViT from scratch is more complex than a standard CNN, but high-level libraries are making them more accessible.\n",
    "\n",
    "This exercise demonstrates the implementation of a ViT, a powerful alternative to CNNs, showcasing the flexibility of modern deep learning architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
