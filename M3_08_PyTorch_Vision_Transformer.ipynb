{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3, Task 8: Vision Transformers in PyTorch\n",
    "\n",
    "**Objective:** Implement and evaluate a Vision Transformer (ViT) model using PyTorch and the `torchvision` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch torchvision matplotlib scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, we'll leverage the `torchvision.models` library, which provides a pre-built, high-quality implementation of the Vision Transformer. Using a pre-built model allows us to focus on the training and evaluation process and often leads to better results than implementing from scratch, especially without extensive hyperparameter tuning. We will fine-tune a ViT pre-trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "IMG_SIZE = 224 # ViT models are often pre-trained on 224x224 images\n",
    "BATCH_SIZE = 32 # Use a smaller batch size for large models\n",
    "\n",
    "# Use transforms expected by pre-trained models\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = torchvision.datasets.EuroSAT(root='./data', download=True, transform=transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "NUM_CLASSES = len(full_dataset.classes)\n",
    "CLASS_NAMES = full_dataset.classes\n",
    "print(f\"Data pipelines ready. Using {NUM_CLASSES} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Pre-trained ViT Model\n",
    "\n",
    "We'll load a `vit_b_16` model pre-trained on ImageNet. We then need to replace the final classification head with a new one tailored to our number of classes (10 for EuroSAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ViT model\n",
    "vit_model = models.vision_transformer.vit_b_16(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Freeze all the parameters in the model\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier head\n",
    "num_features = vit_model.heads.head.in_features\n",
    "vit_model.heads.head = nn.Linear(num_features, NUM_CLASSES)\n",
    "\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "print(\"ViT model loaded and classifier head replaced for fine-tuning.\")\n",
    "# print(vit_model) # Uncomment to see the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Fine-Tuning\n",
    "Since we froze the feature extraction layers, we are only training the weights of the new classification head. This is a form of transfer learning called \"fine-tuning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10 # Fine-tuning often requires fewer epochs\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Only optimize the parameters of the new classifier head\n",
    "optimizer = optim.Adam(vit_model.heads.head.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        history['train_loss'].append(running_loss / total)\n",
    "        history['train_acc'].append(correct / total)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} Val\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        history['val_loss'].append(running_loss / total)\n",
    "        history['val_acc'].append(correct / total)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -> Train Acc: {history['train_acc'][-1]:.4f}, Val Acc: {history['val_acc'][-1]:.4f}\")\n",
    "    return history\n",
    "\n",
    "history = train_and_validate(vit_model, train_loader, val_loader, criterion, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Let's evaluate our fine-tuned ViT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_acc'], label='Training Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('ViT Accuracy (Fine-tuned)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('ViT Loss (Fine-tuned)')\n",
    "plt.show()\n",
    "\n",
    "# Detailed Classification Report\n",
    "vit_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"\\nFine-tuned ViT Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "By using a pre-trained Vision Transformer and fine-tuning it on our specific task, we can achieve very high performance with significantly less training time and data compared to training from scratch. The features learned by the ViT on the massive ImageNet dataset are highly transferable to our EuroSAT land classification problem. This demonstrates the power and efficiency of transfer learning, which is a cornerstone of modern computer vision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
